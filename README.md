# PyWhiz üßô‚Äç‚ôÇÔ∏è
A simple Python interface for AI models. PyWhiz eliminates boilerplate code, letting you make quick one-off calls or maintain stateful chat sessions with conversation history‚Äîclean, fast, and easy to use.

**Please follow the installation steps carefully and take a moment to see how to use the package.**

## üìñ Table of Contents

1. üöÄ [Installation](#-installation)
2. üîπ [Understand the Package Return](#0-understand-the-package-return)
3. ü¶ô [Ollama Information](#1-single-stateless-calls-in-ollama)
4. üìù [Gemini Information](#1-single-stateless-calls-in-geminiJ)
5. üôå [Contributing](#-contributing)
6. üìú [License](#-license)



## ‚ú® Key Features
* **Stateless & Stateful:** Make a single request (prompt) or create a session that automatically remembers conversation history.
* **Developer-Friendly:** Designed to be easy to use and integrate into your existing projects.
* **Current Support:**
    * [Ollama](https://ollama.ai/)

## üöÄ Installation

### If the package was published (Not yet):

```bash
pip install PyWhiz
```

### How to Install the Project Locally

### 1. Clone the Project
First, clone the repository to your computer:
```bash
git clone https://github.com/FilipeAlcaide04/PyWhiz
cd PyWhiz
```

### 2. Build the Package

Build the Python package from the project root folder:

Windows:
```bash
python -m build
```
Linux or MacOS:
```bash
python3 -m build
```
* This will create a folder called dist/ containing the .whl file.

### 3. Install the Package Locally

Use pip to install the package from the .whl file. Replace the path with the actual location of the file on your system:
```bash
pip install /path/to/your/project/dist/pywhiz-0.1.2-py3-none-any.whl
```

# üí° How to Use
PyWhiz provides two main ways to interact with Ollama: single stateless calls and stateful sessions with history.




---
### 0. Understand the package return

When using statefull chat the package returns a dataclass:

The AIResponse object makes it easy to access all information about the AI‚Äôs reply:

| Field         | What it contains                                                    |
| ------------- | ------------------------------------------------------------------- |
| `prompt`      | Your input text sent to the AI                                      |
| `response`    | Text generated by the AI                                            |
| `model`       | The model used for this response                                    |
| `history`     | List of previous turns (`prompt` + `response`) **before this call** |
| `latency`     | Time in seconds it took to get the response                         |
| `tokens_used` | Optional: number of tokens the model used                           |
| `error`       | If something went wrong, contains an error message                  |

**Example to access parts of the response:**

```python
print(res2.prompt)    # The question you sent
print(res2.response)  # The AI's answer
print(res2.latency)   # How long it took
print(res2.history)   # Previous conversation
```

**Get full session history:**
``` python
for turn in session.get_history():
    print(f"User asked: {turn.prompt}")
    print(f"AI answered: {turn.response}")
```


**Clear session history if needed:**

``` python
session.clear_history()
```

### ‚úÖ Summary:

* Use **call_ollama** for quick single answers (returns a string).

* Use **OllamaSession** for conversational AI with memory (returns an AIResponse object).

* The AIResponse dataclass contains everything: your prompt, the AI‚Äôs answer, conversation history, latency, and errors ‚Äî you can pick what you need.




---

### 1. Single, Stateless Calls in Ollama
Use this when you just want a single answer from the model.

```python
from pywhiz import call_ollama

response = call_ollama(
    prompt="What is the capital of Portugal?",
    api_url="http://localhost:11434/api/generate",
    model="gemma:2b"
)

print(response)  # This prints the AI's answer
````
* **Input (prompt)**: The question or text you want the AI to respond to.

* **Return**: A string containing the AI‚Äôs response.
* **No history** is stored here ‚Äî it‚Äôs just one-off.

___

### 2. Stateful Chat Sessions with History in Ollama
Use this when you want the AI to remember the conversation.

```python
from pywhiz import OllamaSession

session = OllamaSession(model="gemma:2b")

# First message
res1 = session.ask("My name is Filipe and I live in Lisbon.")
print(res1.response)

# Second message (AI remembers the first)
res2 = session.ask("Where do I live?")
print(res2.response)
```

## üôå Contributing
Contributions are welcome! Whether it‚Äôs a bug report, feature suggestion, or pull request, your help is appreciated. Feel free to open an issue or submit a PR.

## üìú License
This project is licensed under the MIT License.

